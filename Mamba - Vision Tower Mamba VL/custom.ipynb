{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week_03_page_117.png\n"
     ]
    }
   ],
   "source": [
    "item = {'Image_Path': '/scratch/faaraan/LLaVAData/images/week_03/week_03_page_117.png'}\n",
    "\n",
    "# Extract the filename\n",
    "image_id = item['Image_Path'].split('/')[-1]\n",
    "\n",
    "print(image_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "# Load the JSON data\n",
    "with open('Final_weeks_QnA.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "image_folder = '/scratch/faaraan/LLaVAData/images/'\n",
    "\n",
    "Q_A = pd.read_json(\"Final_weeks_QnA.json\")   \n",
    "Q_A['Image_Path'] = [os.path.join(image_folder, f\"week_{week:02d}/week_{week:02d}_page_{page:03d}.png\") for week, page in zip(Q_A['week'], Q_A['page'])]\n",
    "# Define the output data structure\n",
    "result = []\n",
    "\n",
    "# Iterate through each item in the input JSON\n",
    "for i, item in Q_A.iterrows():\n",
    "    # Generate the ID and image path\n",
    "    # id_ = f\"week_{item['week']:02d}_page_{item['page']:03d}\"\n",
    "    # image = f\"week_{item['week']:02d}/week_{item['week']:02d}_page_{item['page']:03d}.png\"\n",
    "    \n",
    "    # if item['page'] < 10:\n",
    "    #     image = f'week_03/week_03_page_00{item[\"page\"]}.png'\n",
    "    #     id_ = f'week_03_page_00{item[\"page\"]}.png'\n",
    "    # elif item['page'] > 100:\n",
    "    #     image = f'week_03/week_03_page_{item[\"page\"]}.png'\n",
    "    #     id_ = f'week_03_page_{item[\"page\"]}.png'\n",
    "\n",
    "    # else:\n",
    "    #     image = f'week_03/week_03_page_0{item[\"page\"]}.png'\n",
    "    #     id_ = f'week_03_page_0{item[\"page\"]}.png'\n",
    "\n",
    "    image = item['Image_Path']\n",
    "    id_ = item['Image_Path'].split('/')[-1]\n",
    "\n",
    "    # Construct the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"from\": \"human\",\n",
    "            \"value\": f\"<image>\\n{item['instruction']}\"\n",
    "        },\n",
    "        {\n",
    "            \"from\": \"gpt\",\n",
    "            \"value\": f\"{item['response']}\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Append the conversation to the result\n",
    "    result.append({\"id\": id_, \"image\": image, \"conversations\": conversation})\n",
    "\n",
    "# Write the result to a new JSON file\n",
    "with open('result_data_llava.json', 'w') as f:\n",
    "    json.dump(result, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: six in c:\\users\\faara\\anaconda3\\lib\\site-packages (from rouge) (1.16.0)\n",
      "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: rouge\n",
      "Successfully installed rouge-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueError: Hypothesis is empty. - Assigning zero scores.\n",
      "ValueError: Hypothesis is empty. - Assigning zero scores.\n",
      "ValueError: Hypothesis is empty. - Assigning zero scores.\n",
      "ValueError: Hypothesis is empty. - Assigning zero scores.\n",
      "Skipping empty prediction or ground truth for question: What operations are discussed in the transcript?\n",
      "Skipping empty prediction or ground truth for question: What operations are outlined in the transcript?\n",
      "ValueError: Hypothesis is empty. - Assigning zero scores.\n",
      "ValueError: Hypothesis is empty. - Assigning zero scores.\n",
      "ValueError: Hypothesis is empty. - Assigning zero scores.\n",
      "ValueError: Hypothesis is empty. - Assigning zero scores.\n",
      "Average ROUGE-1 score: 0.31247366398542326\n",
      "Average ROUGE-2 score: 0.13591904197166196\n",
      "Average ROUGE-L score: 0.28565209414172926\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from rouge import Rouge\n",
    "\n",
    "# Load JSON data\n",
    "input_file = 'predictions_week_1_to_8.json'  # Adjust the path to your JSON file\n",
    "with open(input_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Clean the predicted responses\n",
    "for item in data:\n",
    "    item[\"Predicted\"] = item[\"Predicted\"].replace(\"\\n\", \"\").replace(\"\\r\", \"\").strip()\n",
    "    # Clean other unwanted characters or text here as needed\n",
    "    item[\"Predicted\"] = item[\"Predicted\"].replace(\"\\u0131\", \"\")\n",
    "    item[\"Predicted\"] = item[\"Predicted\"].replace(\"\\u0627\", \"\")\n",
    "    item[\"Predicted\"] = item[\"Predicted\"].replace(\"\\u0646\", \"\")\n",
    "    item[\"Predicted\"] = item[\"Predicted\"].replace(\n",
    "        \"\\u0627\\u0646\\u0635\\u062d \\u0628\\u062a\\u062e\\u0641\\u064a\\u0641 \\u0627\\u0644\\u0635\\u0648\\u0631\\u0629 \\u0645\\u0646\\u0639\\u062f \\u0627\\u0644\\u0642\\u064a\\u0627\\u0645 \\u0628\\u0639\\u0645\\u0644 \\u0627\\u0644\\u0646\\u0635\\u062d \\u0628\\u062a\\u062e\\u0641\\u064a\\u0641 \\u0627\\u0644\\u0635\\u0648\\u0631\\u0629 \\u0628\\u0639\\u062f \\u0627\\u0644\\u0642\\u064a\\u0627\\u0645 \\u0628\\u0639\\u0645\\u0644\\u064a\\u0629 \\u0639\\u0646\\u0642\\u064a\\u0636\\n\\n\", \"\")\n",
    "\n",
    "# Function to calculate ROUGE scores\n",
    "def calculate_rouge(predicted_text, ground_truth_text):\n",
    "    rouge = Rouge()\n",
    "    try:\n",
    "        scores = rouge.get_scores(predicted_text, ground_truth_text)\n",
    "        rouge_1_score = scores[0][\"rouge-1\"][\"f\"]\n",
    "        rouge_2_score = scores[0][\"rouge-2\"][\"f\"]\n",
    "        rouge_l_score = scores[0][\"rouge-l\"][\"f\"]\n",
    "    except ValueError as e:\n",
    "        print(f\"ValueError: {e} - Assigning zero scores.\")\n",
    "        rouge_1_score, rouge_2_score, rouge_l_score = 0.0, 0.0, 0.0\n",
    "    return rouge_1_score, rouge_2_score, rouge_l_score\n",
    "\n",
    "# Initialize list to store ROUGE scores\n",
    "rouge_scores = []\n",
    "\n",
    "# Iterate over the data and calculate ROUGE scores\n",
    "for entry in data:\n",
    "    question = entry['Question']\n",
    "    original_response = entry['Ground Truth'].strip()\n",
    "    predicted_response = entry['Predicted'].strip()\n",
    "    \n",
    "    # Skip if the predicted or ground truth response is empty\n",
    "    if not predicted_response or not original_response:\n",
    "        print(f\"Skipping empty prediction or ground truth for question: {question}\")\n",
    "        continue\n",
    "\n",
    "    # Calculate ROUGE scores with exception handling\n",
    "    rouge_1, rouge_2, rouge_l = calculate_rouge(predicted_response, original_response)\n",
    "    rouge_scores.append({\n",
    "        'Question': question,\n",
    "        'Ground Truth': original_response,\n",
    "        'Predicted': predicted_response,\n",
    "        'ROUGE-1': rouge_1,\n",
    "        'ROUGE-2': rouge_2,\n",
    "        'ROUGE-L': rouge_l\n",
    "    })\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "rouge_df = pd.DataFrame(rouge_scores)\n",
    "\n",
    "# Calculate average ROUGE scores\n",
    "average_rouge_scores = rouge_df[['ROUGE-1', 'ROUGE-2', 'ROUGE-L']].mean()\n",
    "\n",
    "# Print average ROUGE scores\n",
    "print(\"Average ROUGE-1 score:\", average_rouge_scores['ROUGE-1'])\n",
    "print(\"Average ROUGE-2 score:\", average_rouge_scores['ROUGE-2'])\n",
    "print(\"Average ROUGE-L score:\", average_rouge_scores['ROUGE-L'])\n",
    "\n",
    "# Optionally, save the DataFrame with ROUGE scores to a new JSON file\n",
    "# output_file = '/path/to/results_with_rouge_scores.json'  # Adjust the path to your output JSON file\n",
    "# rouge_df.to_json(output_file, orient='records', indent=4)\n",
    "# print(f\"Results with ROUGE scores saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-1 score: 0.37998290227389137\n",
      "Average ROUGE-2 score: 0.1698322163495669\n",
      "Average ROUGE-L score: 0.33219930514546675\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from rouge import Rouge\n",
    "\n",
    "# Load JSON data\n",
    "input_file = 'Sporrow_test_result_32_stage_4_epochs.json'  # Adjust the path to your JSON file\n",
    "with open(input_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Remove newline characters from the predicted_response fields\n",
    "for item in data:\n",
    "    item[\"predicted_response\"] = item[\"Predicted_Answer\"].replace(\"\\n\", \"\")\n",
    "\n",
    "# Function to calculate ROUGE scores\n",
    "def calculate_rouge(predicted_text, ground_truth_text):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(predicted_text, ground_truth_text)\n",
    "    rouge_1_score = scores[0][\"rouge-1\"][\"f\"]\n",
    "    rouge_2_score = scores[0][\"rouge-2\"][\"f\"]\n",
    "    rouge_l_score = scores[0][\"rouge-l\"][\"f\"]\n",
    "    return rouge_1_score, rouge_2_score, rouge_l_score\n",
    "\n",
    "# Initialize list to store ROUGE scores\n",
    "rouge_scores = []\n",
    "\n",
    "# Iterate over the data and calculate ROUGE scores\n",
    "for entry in data:\n",
    "    question = entry['instruction']\n",
    "    original_response = entry['Original_Answer']\n",
    "    predicted_response = entry['predicted_response']\n",
    "    \n",
    "    rouge_1, rouge_2, rouge_l = calculate_rouge(predicted_response, original_response)\n",
    "    rouge_scores.append({\n",
    "        'question': question,\n",
    "        'original_response': original_response,\n",
    "        'predicted_response': predicted_response,\n",
    "        'ROUGE-1': rouge_1,\n",
    "        'ROUGE-2': rouge_2,\n",
    "        'ROUGE-L': rouge_l\n",
    "    })\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "rouge_df = pd.DataFrame(rouge_scores)\n",
    "\n",
    "# Calculate average ROUGE scores\n",
    "average_rouge_scores = rouge_df[['ROUGE-1', 'ROUGE-2', 'ROUGE-L']].mean()\n",
    "\n",
    "# Print average ROUGE scores\n",
    "print(\"Average ROUGE-1 score:\", average_rouge_scores['ROUGE-1'])\n",
    "print(\"Average ROUGE-2 score:\", average_rouge_scores['ROUGE-2'])\n",
    "print(\"Average ROUGE-L score:\", average_rouge_scores['ROUGE-L'])\n",
    "\n",
    "# Optionally, save the DataFrame with ROUGE scores to a new JSON file\n",
    "# output_file = '/path/to/results_with_rouge_scores.json'  # Adjust the path to your output JSON file\n",
    "# rouge_df.to_json(output_file, orient='records', indent=4)\n",
    "# print(f\"Results with ROUGE scores saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
